{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2483489-d4cd-4cbf-92ce-8de418907f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching article titles...\n",
      "Fetched 500 article titles.\n",
      "Fetched 500 articles. Fetching content...\n",
      "\n",
      "|██████████████████████████████████████████████████| 100% Elapsed: 0m 28s ETA: 0m 0s\n",
      "\n",
      "Pipeline completed successfully.\n"
     ]
    }
   ],
   "source": [
    "import aiohttp\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "import re\n",
    "import csv\n",
    "import os\n",
    "from collections import Counter\n",
    "from bs4 import BeautifulSoup\n",
    "from classes.progress_tracker import ProgressTracker\n",
    "\n",
    "class WikipediaScraper:\n",
    "    def __init__(self, api_url=\"https://ms.wikipedia.org/w/api.php\", max_concurrent_requests=10):\n",
    "        self.api_url = api_url\n",
    "        self.session = None\n",
    "        self.semaphore = asyncio.Semaphore(max_concurrent_requests)  # Limit concurrency\n",
    "\n",
    "    async def __aenter__(self):\n",
    "        self.session = aiohttp.ClientSession()\n",
    "        return self\n",
    "\n",
    "    async def __aexit__(self, exc_type, exc_val, exc_tb):\n",
    "        await self.session.close()\n",
    "\n",
    "    async def fetch_article_titles(self, limit=50):\n",
    "        \"\"\"Fetch recent article titles from Wikipedia.\"\"\"\n",
    "        print(\"Fetching article titles...\")\n",
    "        search_term = \"history\"\n",
    "        params = {\n",
    "            \"action\": \"query\",         # We use the \"query\" action\n",
    "            \"list\": \"search\",         # Search query\n",
    "            \"srsearch\": search_term,  # The search term to query\n",
    "            \"srlimit\": \"max\",         # Max number of results (you can adjust this as per your needs)\n",
    "            \"format\": \"json\"          # Return the data in JSON format\n",
    "        }\n",
    "        try:\n",
    "            async with self.semaphore:\n",
    "                async with self.session.get(self.api_url, params=params) as response:\n",
    "                    if response.status == 200:\n",
    "                        data = await response.json()\n",
    "                        articles = data['query']['search']\n",
    "                        seen_titles = set()\n",
    "                        article_info = []\n",
    "                        for article in articles:\n",
    "                            title = article['title']\n",
    "                            if title not in seen_titles:\n",
    "                                link = f\"https://ms.wikipedia.org/wiki/{title.replace(' ', '_')}\"\n",
    "                                article_info.append((title, link))\n",
    "                                seen_titles.add(title)\n",
    "                            if len(article_info) >= limit:\n",
    "                                break\n",
    "                        print(f\"Fetched {len(article_info)} article titles.\")\n",
    "                        return article_info\n",
    "                    else:\n",
    "                        print(\"Error fetching data from Wikipedia API.\")\n",
    "                        return []\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching articles: {e}\")\n",
    "            return []\n",
    "\n",
    "    async def fetch_article_content(self, title):\n",
    "        \"\"\"Fetch the content of a specific Wikipedia article.\"\"\"\n",
    "        url = f\"https://ms.wikipedia.org/wiki/{title.replace(' ', '_')}\"\n",
    "        async with self.semaphore:\n",
    "            async with self.session.get(url) as response:\n",
    "                if response.status == 200:\n",
    "                    soup = BeautifulSoup(await response.text(), 'html.parser')\n",
    "                    content_div = soup.find(id=\"bodyContent\")\n",
    "                    if content_div:\n",
    "                        paragraphs = content_div.find_all('p')\n",
    "                        return ' '.join([para.get_text() for para in paragraphs])\n",
    "        return \"\"\n",
    "\n",
    "class ArticleProcessor:\n",
    "    @staticmethod\n",
    "    def clean_text(text):\n",
    "        \"\"\"Clean the text by removing special characters and converting to lowercase.\"\"\"\n",
    "        return re.sub(r'[^a-zA-Z\\s]', ' ', text).lower().strip()\n",
    "\n",
    "    @staticmethod\n",
    "    def save_word_counts_to_csv(word_counts, filename=\"./raw_data/word_counts.csv\"):\n",
    "        directory = os.path.dirname(filename)\n",
    "        if directory and not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "        sorted_word_counts = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "        with open(filename, mode='w', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\"word\", \"count\"])\n",
    "            for word, count in sorted_word_counts:\n",
    "                writer.writerow([word, count])\n",
    "\n",
    "    @staticmethod\n",
    "    def filter_word_counts(word_counts, min_count=3, min_length=2):\n",
    "        \"\"\"Filter word counts based on minimum count and word length.\"\"\"\n",
    "        return {word: count for word, count in word_counts.items() if count > min_count and len(word) >= min_length}\n",
    "\n",
    "class WikipediaPipeline:\n",
    "    def __init__(self, api_url=\"https://ms.wikipedia.org/w/api.php\", limit=200, max_concurrent_requests=10):\n",
    "        self.api_url = api_url\n",
    "        self.limit = limit\n",
    "        self.max_concurrent_requests = max_concurrent_requests\n",
    "\n",
    "    async def run(self):\n",
    "        async with WikipediaScraper(self.api_url, self.max_concurrent_requests) as scraper:\n",
    "            articles = await scraper.fetch_article_titles(self.limit)\n",
    "\n",
    "            if not articles:\n",
    "                print(\"No articles fetched. Exiting...\")\n",
    "                return\n",
    "\n",
    "            print(f\"Fetched {len(articles)} articles. Fetching content...\")\n",
    "\n",
    "            # Initialize progress tracker\n",
    "            progress_tracker = ProgressTracker(len(articles))\n",
    "\n",
    "            async def fetch_content_with_progress(title):\n",
    "                content = await scraper.fetch_article_content(title)\n",
    "                progress_tracker.update()\n",
    "                return content\n",
    "\n",
    "            tasks = [fetch_content_with_progress(title) for title, _ in articles]\n",
    "            all_content = await asyncio.gather(*tasks)\n",
    "\n",
    "            progress_tracker.complete()\n",
    "\n",
    "            # Process and count words\n",
    "            overall_word_counts = Counter()\n",
    "            for content in all_content:\n",
    "                if content:\n",
    "                    cleaned_text = ArticleProcessor.clean_text(content)\n",
    "                    words = cleaned_text.split()\n",
    "                    overall_word_counts.update(words)\n",
    "\n",
    "            # Save filtered results\n",
    "            filtered_word_counts = ArticleProcessor.filter_word_counts(overall_word_counts)\n",
    "            ArticleProcessor.save_word_counts_to_csv(filtered_word_counts)\n",
    "            print(\"Pipeline completed successfully.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    nest_asyncio.apply()\n",
    "    pipeline = WikipediaPipeline(api_url=\"https://ms.wikipedia.org/w/api.php\", limit=500, max_concurrent_requests=10)\n",
    "    asyncio.get_event_loop().run_until_complete(pipeline.run())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474a4dbf-41a6-4836-9ee3-bde3338f0c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating 150 words...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import aiohttp\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "import nltk\n",
    "import time\n",
    "from nltk.corpus import words\n",
    "from pyspark.sql import SparkSession\n",
    "from bs4 import BeautifulSoup\n",
    "from classes.progress_tracker import ProgressTracker\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "try:\n",
    "    words_list = set(words.words())\n",
    "except LookupError:\n",
    "    print(\"Downloading nltk 'words' corpus...\")\n",
    "    nltk.download('words')\n",
    "    words_list = set(words.words())\n",
    "\n",
    "class WordValidator:\n",
    "    def __init__(self, csv_file_path, output_csv_path, neither_csv_path, batch_size=100, reduced_words_count=None):\n",
    "        self.csv_file_path = csv_file_path\n",
    "        self.output_csv_path = output_csv_path\n",
    "        self.neither_csv_path = neither_csv_path\n",
    "        self.batch_size = batch_size\n",
    "        self.reduced_words_count = reduced_words_count  # New parameter for limiting words count\n",
    "        self.spark = SparkSession.builder.appName(\"WordValidator\").getOrCreate()\n",
    "\n",
    "    def load_csv(self):\n",
    "        \"\"\"Load the CSV file into a Spark DataFrame.\"\"\"\n",
    "        df = self.spark.read.csv(self.csv_file_path, header=True, inferSchema=True)\n",
    "        if self.reduced_words_count:\n",
    "            # Limit the number of rows to `reduced_words_count` and retain the count column\n",
    "            if \"count\" in df.columns:\n",
    "                df = df.orderBy(\"count\", ascending=False).limit(self.reduced_words_count)\n",
    "            else:\n",
    "                print(f\"Limiting to the first {self.reduced_words_count} rows.\")\n",
    "                df = df.limit(self.reduced_words_count)\n",
    "        return df\n",
    "\n",
    "    async def check_english_word(self, word, session):\n",
    "        \"\"\"Check if a word is valid in English using an online dictionary API.\"\"\"\n",
    "        if word.lower() in words_list:\n",
    "            return True\n",
    "        try:\n",
    "            url = f\"https://api.dictionaryapi.dev/api/v2/entries/en/{word}\"\n",
    "            async with session.get(url) as response:\n",
    "                return response.status == 200\n",
    "        except Exception:\n",
    "            return False\n",
    "\n",
    "    async def check_malay_word(self, word, session):\n",
    "        \"\"\"Check if a word is valid in Malay using the DBP API.\"\"\"\n",
    "        try:\n",
    "            url = f\"https://prpm.dbp.gov.my/Cari1?keyword={word}\"\n",
    "            async with session.get(url) as response:\n",
    "                soup = BeautifulSoup(await response.text(), 'html.parser')\n",
    "                panel_result = soup.find(id=\"MainContent_panelresult\")\n",
    "                return panel_result and \"<b>Kamus Bahasa Melayu</b>\" in str(panel_result)\n",
    "        except Exception:\n",
    "            return False\n",
    "\n",
    "    async def process_words(self, words, progress_tracker):\n",
    "        \"\"\"Validate a batch of words asynchronously.\"\"\"\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            english_results = await asyncio.gather(\n",
    "                *[self.check_english_word(word, session) for word in words]\n",
    "            )\n",
    "            malay_results = await asyncio.gather(\n",
    "                *[self.check_malay_word(word, session) for word in words]\n",
    "            )\n",
    "        for _ in words:\n",
    "            progress_tracker.update()\n",
    "        return english_results, malay_results\n",
    "\n",
    "    async def validate_words_in_batches(self, word_list, progress_tracker):\n",
    "        \"\"\"Validate words in batches to optimize API calls.\"\"\"\n",
    "        valid_english_results = []\n",
    "        valid_malay_results = []\n",
    "\n",
    "        for i in range(0, len(word_list), self.batch_size):\n",
    "            batch_words = word_list[i:i + self.batch_size]\n",
    "            english_results, malay_results = await self.process_words(batch_words, progress_tracker)\n",
    "            valid_english_results.extend(english_results)\n",
    "            valid_malay_results.extend(malay_results)\n",
    "\n",
    "        return valid_english_results, valid_malay_results\n",
    "\n",
    "    async def validate_and_save(self):\n",
    "        \"\"\"Main function to load, validate, and save the filtered words.\"\"\"\n",
    "        # Load words from CSV\n",
    "        df = self.load_csv()\n",
    "        word_list = df.select(\"word\", \"count\").rdd.map(lambda row: (row['word'], int(row['count']))).collect()\n",
    "\n",
    "        # Initialize progress tracker\n",
    "        print(f\"Validating {len(word_list)} words...\")\n",
    "        progress_tracker = ProgressTracker(len(word_list))\n",
    "\n",
    "        # Separate words and counts\n",
    "        words, counts = zip(*word_list)\n",
    "\n",
    "        # Validate words\n",
    "        valid_english_results, valid_malay_results = await self.validate_words_in_batches(words, progress_tracker)\n",
    "\n",
    "        # Complete progress bar\n",
    "        progress_tracker.complete()\n",
    "        # Create a DataFrame with validation results\n",
    "        validated_df = self.spark.createDataFrame(\n",
    "            zip(words, counts, valid_english_results, valid_malay_results),\n",
    "            [\"word\", \"count\", \"is_english\", \"is_malay\"]\n",
    "        )\n",
    "\n",
    "        # Convert Spark DataFrame to Pandas DataFrame\n",
    "        pandas_df = validated_df.toPandas()\n",
    "\n",
    "        # Filter valid English and Malay words\n",
    "        valid_words_df = pandas_df[(pandas_df['is_english']) | (pandas_df['is_malay'])]\n",
    "        neither_words_df = pandas_df[~(pandas_df['is_english'] | pandas_df['is_malay'])]\n",
    "\n",
    "        valid_words_df.head()\n",
    "\n",
    "        # Save results directly to CSV files\n",
    "        valid_words_df.to_csv(self.output_csv_path, index=False)\n",
    "        neither_words_df.to_csv(self.neither_csv_path, index=False)\n",
    "\n",
    "        print(\"All files saved successfully as single CSV files.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    csv_file_path = \"./raw_data/word_counts.csv\"\n",
    "    output_csv_path = \"./raw_data/valid_word_counts.csv\"\n",
    "    neither_csv_path = \"./raw_data/neither_words.csv\"\n",
    "    validator = WordValidator(csv_file_path, output_csv_path, neither_csv_path, reduced_words_count=200)\n",
    "\n",
    "    async def main():\n",
    "        await validator.validate_and_save()\n",
    "\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61de891e-786a-4c4a-89e8-3c1285a3bb1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Definition: organ berwarna perang kemerah-merahan di dlm tubuh yg mengeluarkan hempedu dan membersihkan darah\n",
      "Examples:\n",
      "Example 1: ~ kambing\n",
      "Example 2: ~ lembu.\n",
      "\n",
      "Definition: guru\n",
      "Examples:\n",
      "Example 1: dia ingin menjadi ~\n",
      "\n",
      "Definition: enak (pd rasa, bau, dll)\n",
      "Examples:\n",
      "Example 1: kuih itu sangat ~ rasanya\n",
      "\n",
      "Definition: balai atau ruang tempat diadakan majlis-majlis forum (tari-menari, perbahasan dll)\n",
      "Examples:\n",
      "Example 1: Forum itu telah diadakan di ~ Canselor, Universiti Malaya.\n",
      "\n",
      "Definition: air yg turun (menitik dgn banyaknya) yg terpeluwap drpd wap di atmosfera\n",
      "Examples:\n",
      "Example 1: pagi ini ~ turun dgn lebatnya\n",
      "Example 2: musim ~ = ketika ~ musim yg hujan selalu turun\n",
      "\n",
      "Definition: menyampaikan ilmu pengetahuan (kemahiran dsb) kpd orang lain, mendidik, melatih, memberikan petunjuk-petun­juk kpd\n",
      "Examples:\n",
      "Example 1: ~ orang menggunakan senjata\n",
      "\n",
      "Definition: tidak sempit, lapang (pakaian, lubang, dll)\n",
      "Examples:\n",
      "Example 1: seluar baru itu ~ sedikit\n",
      "\n",
      "Definition: ; ~ kurus sj tumbuhan (pokok), berasberas, kasir, kedondong matahari, kempas roman, pauh kijang, Trigonochlamys griffithii; rumput ~ sj tumbuhan (rumpai), getang-getang, ketumbi padang, tutup bumi paya, Synedrella nodiflora.\n",
      "Examples:\n",
      "None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class DefinitionExampleEntry:\n",
    "    def __init__(self, definition: str, examples: list[str]):\n",
    "        \"\"\"Initialize a DefinitionExampleEntry object.\n",
    "\n",
    "        Args:\n",
    "            definition (str): The definition extracted from the text.\n",
    "            examples (list[str]): A list of examples extracted from the text.\n",
    "        \"\"\"\n",
    "        self.definition = definition\n",
    "        self.examples = examples\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"Return a string representation of the object.\"\"\"\n",
    "        return self.__str__()\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\"Return a user-friendly string representation of the object.\"\"\"\n",
    "        examples_str = \"\\n\".join(self.examples)\n",
    "        return (\n",
    "            f\"Definition: {self.definition}\\nExamples:\\n{examples_str if examples_str else 'None'}\"\n",
    "        )\n",
    "\n",
    "def extract_definition_and_examples(text: str) -> DefinitionExampleEntry:\n",
    "    \"\"\"Extract definition and examples from the input text.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text containing a definition and examples.\n",
    "\n",
    "    Returns:\n",
    "        DefinitionExampleEntry: An object containing the definition and examples.\n",
    "    \"\"\"\n",
    "    # Split the input into definition and examples based on the first colon\n",
    "    parts = text.split(\":\", 1)\n",
    "\n",
    "    # The first part is always the definition\n",
    "    definition = parts[0].strip()\n",
    "\n",
    "    # Check if examples exist (based on the presence of a second part)\n",
    "    if len(parts) > 1:\n",
    "        examples_text = parts[1]\n",
    "        examples = [\n",
    "            f\"Example {i + 1}: {example.strip()}\"\n",
    "            for i, example in enumerate(examples_text.split(\";\"))\n",
    "            if example.strip()\n",
    "        ]\n",
    "    else:\n",
    "        examples = []\n",
    "\n",
    "    # Return an instance of DefinitionExampleEntry\n",
    "    return DefinitionExampleEntry(definition, examples)\n",
    "\n",
    "test_texts = [\n",
    "    \"organ berwarna perang kemerah-merahan di dlm tubuh yg mengeluarkan hempedu dan membersihkan darah: ~ kambing; ~ lembu.\", #hati\n",
    "    \"guru: dia ingin menjadi ~;\", #cikgu\n",
    "    \"enak (pd rasa, bau, dll): kuih itu sangat ~ rasanya;\", #sedap\n",
    "    \"balai atau ruang tempat diadakan majlis-majlis forum (tari-menari, perbahasan dll): Forum itu telah diadakan di ~ Canselor, Universiti Malaya.\", #dewan\n",
    "    \" air yg turun (menitik dgn banyaknya) yg terpeluwap drpd wap di atmosfera: pagi ini ~ turun dgn lebatnya; musim ~ = ketika ~ musim yg hujan selalu turun;\", #hujan\n",
    "    \"menyampaikan ilmu pengetahuan (kemahiran dsb) kpd orang lain, mendidik, melatih, memberikan petunjuk-petun­juk kpd: ~ orang menggunakan senjata;\", #ajar\n",
    "    \"tidak sempit, lapang (pakaian, lubang, dll): seluar baru itu ~ sedikit;\", #langgar\n",
    "    \"; ~ kurus sj tumbuhan (pokok), berasberas, kasir, kedondong matahari, kempas roman, pauh kijang, Trigonochlamys griffithii; rumput ~ sj tumbuhan (rumpai), getang-getang, ketumbi padang, tutup bumi paya, Synedrella nodiflora.\" #babi2\n",
    "]\n",
    "\n",
    "# Process each text and print the result\n",
    "for text in test_texts:\n",
    "    entry = extract_definition_and_examples(text)\n",
    "    print(entry)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "de-ass",
   "language": "python",
   "name": "de-ass"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
